{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663321ce-fbee-4dd1-b5b9-dc3e1d216433",
   "metadata": {},
   "source": [
    "# Open Library Subject Analysis from 1900s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26008e48-3639-49d4-a132-ea2a00843a7c",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This study leverages the Open Library dataset, containing 89,677,650 bibliographic records with metadata on subjects, publication dates, and geographic origins, to explore global publishing trends and subject distributions from the 1900s to the 2020s. Using Apache Spark and Apache Parquet for scalable data processing, the analysis examines publication volumes, country contributions, subject co-occurrences, and temporal trends in subject emergence. Key findings reveal a peak in publication output in the 2000s, dominance by the USA and UK, an average of 5.1 subjects per publication, and universal subjects like history appearing across 249 countries. The data was parsed into editions and expanded subject DataFrames, stored in Parquet format, and transformed by exploding nested arrays for granular analysis. This report demonstrates a pipeline for big data analysis to insights into cultural and intellectual trends in global publishing and recommendations for further research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd5509-f86d-4529-837b-1badd26bab29",
   "metadata": {},
   "source": [
    "## Motivation and Problem Statement \n",
    "\n",
    "Books reflect the cultural and intellectual priorities of the societies in which they are published. However, analyzing such patterns at scale poses significant challenges. The OpenLibrary dataset, while rich in information, is vast and semi-structured, with key fields like subjects, dates, and locations stored in nested array formats. Extracting meaningful trends from this data requires a robust technical pipeline capable of handling large volumes of data, transforming complex structures, and supporting efficient querying and aggregation.\n",
    "\n",
    "This project addresses these challenges by using PySpark and Parquet-based partitioning to preprocess and structure the data in a way that enables scalable analysis. The ultimate goal is to better understand how thematic relationships evolve in the published record—and to demonstrate the critical role of big data tools in making this type of cultural analysis possible at scale.\n",
    "\n",
    "This project aims to uncover temporal and geographic trends in publication activity and subject diversity, addressing questions such as: \n",
    "- How have publication volumes evolved over time? Which countries drive global publishing?\n",
    "- How do subjects co-occur?\n",
    "- How do they vary by region and decade?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90002325-fd34-4b83-9a3c-8067c9ad1547",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "The methodology of this study follows a structured data processing pipeline designed to efficiently manage and analyze the scale and complexity of the OpenLibrary dataset. The pipeline consists of the following key stages:\n",
    "1. **Data Ingestion and Initial Processing** - The dataset is loaded from its source and parsed into distinct components, specifically editions, works, and authors. This separation facilitates modular data handling and enables more efficient querying and transformation in subsequent steps.\n",
    "2. **Parquet Storage and Partitioning** - The parsed data is converted into Apache Parquet format and partitioned by logical keys such as publication year and region. This step is critical for optimizing read performance and significantly reducing overall runtime during analysis.\n",
    "3. **Data Transormation and Exploding Nested Fields** - To prepare the data for granular analysis, nested array fields are exploded. Each subject is represented as a separate row, and publish country fields are flattened and standardized to normalize the dataset and support consistent temporal and geographic aggregation.\n",
    "4. **Trend and Pattern Analysis** - With the data cleaned and structured, subject pairings are extracted and analyzed over time and across regions. Visualizations and statistical summaries are generated to reveal patterns, identify emerging or declining themes, and explore regional differences in subject co-occurrence trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8122f50-5e7e-4a7e-881d-90600987ebb7",
   "metadata": {},
   "source": [
    "## Data Extraction and Processing\n",
    "The Open Library dataset was sourced from a 41.89 GB compressed text file (`ol_cdump_latest.txt.gz`) at `/mnt/data/public/openlibrary/20250426/`. It was processed as follows:\n",
    "\n",
    "### Data Loading\n",
    "1. **Initialization**: A Spark session was initialized with `SparkSession.builder.appName(\"Open Library Dataframes Creation\").master('local[*]').getOrCreate()`, utilizing all local cores.\n",
    "2. **Reading**: The file was read as a text DataFrame using `spark.read.text(file_path)`, initially with one partition (`df.rdd.getNumPartitions()` returned 1).\n",
    "3. **Splitting**: The text was split on tabs using `split(col(\"value\"), \"\\t\").alias(\"fields\")`, creating columns: `record_type`, `key`, `revision`, `timestamp`, and `json_data`.\n",
    "4. **Parsing**: The `json_data` column was parsed using a schema (`editions_json_schema`) with fields like `title`, `authors`, `subjects`, and `publish_country`, applied via `from_json(col(\"json_data\"), editions_json_schema)`.\n",
    "5. **Transformation**: The parsed DataFrame (`df_eds_final`) extracted fields like `edition_key`, `title`, `author_keys`, `language_keys`, `publish_country`, and `subjects`, using `transform` to simplify nested arrays (e.g., extracting author keys).\n",
    "\n",
    "### Partitioning\n",
    "1. **Output**: The transformed DataFrame was written to Parquet at `/partitioned_data/eds_partitioned/` using `df_eds_final.write.parquet`, resulting in 103 partitions. The editions Parquet files totaled 12.96 GB, with authors at 450.11 MB and works at 3.87 GB.\n",
    "2. **Optimization**: Parquet’s columnar storage and partitioning by decade and country (in subsequent analyses) enhanced query performance and compression.\n",
    "\n",
    "### Preprocessing\n",
    "The editions DataFrame (`df_eds_final`) was further processed to create working DataFrames (`df_eds` and `df_explode`):\n",
    "1. **Year Extraction**: A `year` column was derived using `regexp_extract(col(\"publish_date\"), r\"(\\d{4})\", 1)`, setting non-matching values to null.\n",
    "2. **Decade Normalization**: A `decade` column was created with `concat((floor(col(\"year\") / 10) * 10).cast(\"string\"), lit(\"s\"))` for years between 1900 and 2020, otherwise “Unknown”. The DataFrame was filtered to include only 1900–2020 records.\n",
    "3. **Country Standardization**: The `publish_country` column was cleaned with `lower(regexp_replace(col(\"publish_country\"), r\"\\s+\", \" \"))` and joined with a broadcasted `country_mapping` DataFrame to standardize country names, defaulting to “Unknown” for unmapped values.\n",
    "4. **Subject Cleaning**: The `subjects` column was transformed into `subjects_cleaned` by applying `trim(lower(regexp_replace(...)))` to remove commas, slashes, and extra spaces. A `subject_clean_flat` column was created by splitting subjects on commas and flattening arrays, followed by `subject_clean` to filter out invalid entries (e.g., “etc” or trailing spaces) using `array_distinct` and `spark_filter`. Rows with empty `subject_clean` arrays were excluded.\n",
    "5. **Output**: The resulting `df_eds` (columns: `decade`, `country`, `subject_clean`) was written to Parquet at `/partitioned_data/df_eds_partitioned/` with `write.parquet`.\n",
    "6. **Exploding Subjects**: The `df_explode` DataFrame was created by exploding `subject_clean` into individual rows (`subject`) using `explode(col(\"subject_clean\"))`, selecting `decade`, `country`, and `subject`, and filtering non-null subjects. It was written to Parquet at `/partitioned_data/df_exp_partitioned/`.\n",
    "\n",
    "### Final DataFrames\n",
    "- **Editions (`df_eds`)**: Columns: `decade`, `country`, `subject_clean` (array of strings).\n",
    "- **Expanded Subjects (`df_explode`)**: Columns: `decade`, `country`, `subject` (single string per row).\n",
    "\n",
    "### Schema\n",
    "- **Editions**:\n",
    "  - `decade`: String (Nullable = true)\n",
    "  - `country`: String (Nullable = true)\n",
    "  - `subject_clean`: Array of strings (ContainsNull = true)\n",
    "- **Expanded Subjects**:\n",
    "  - `decade`: String (Nullable = true)\n",
    "  - `country`: String (Nullable = true)\n",
    "  - `subject`: String (Nullable = true)\n",
    "\n",
    "### **All the steps above `openlib_partitions.ipynb` and `openlib_working_df_partitions.ipynb`**\n",
    "\n",
    "### Additional Processing\n",
    "- **Cleaning**: Filtered “Unknown” countries for geographic analyses.\n",
    "- **Transformations**: Added `subject_count` using `size(col(\"subject_clean\"))`; created a UDF (`generate_pairs`) for sorted subject pairs (e.g., “subject1|subject2”).\n",
    "- **Aggregations**: Used group-by and window functions for counts and cumulative sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f9d3ee-a849-470c-b33d-c50487b38941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:40:40.104068Z",
     "iopub.status.busy": "2025-05-18T11:40:40.103829Z",
     "iopub.status.idle": "2025-05-18T11:40:42.220596Z",
     "shell.execute_reply": "2025-05-18T11:40:42.219106Z",
     "shell.execute_reply.started": "2025-05-18T11:40:40.104044Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (col, count, countDistinct,\n",
    "                                    sum as sum_, size, min as min_, lit, \n",
    "                                    concat, floor, regexp_extract, round,\n",
    "                                    regexp_replace, lower, trim, transform,\n",
    "                                    split, flatten, array_distinct, when,\n",
    "                                    row_number, explode, udf)\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_colwidth\",None)\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f027e70b-386c-49a2-9328-2f4b6fdbdb5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:40:42.227394Z",
     "iopub.status.busy": "2025-05-18T11:40:42.227185Z",
     "iopub.status.idle": "2025-05-18T11:40:48.505973Z",
     "shell.execute_reply": "2025-05-18T11:40:48.504778Z",
     "shell.execute_reply.started": "2025-05-18T11:40:42.227374Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Open Library Subject Analyses\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.driver.memory\", \"16g\")\n",
    "         .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "         .config(\"spark.executor.memory\", \"8g\")\n",
    "         .config(\"spark.executor.cores\", \"4\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "         .config(\"spark.default.parallelism\", \"32\")\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "         .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "         .config(\"spark.memory.offHeap.size\", \"4g\")\n",
    "         .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "         .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d997a2-a2ec-41c4-9a0d-31ff43922dbc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0406d22a-6d7c-4f1a-ac4b-08a61a8fceef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:40:48.508314Z",
     "iopub.status.busy": "2025-05-18T11:40:48.507524Z",
     "iopub.status.idle": "2025-05-18T11:40:52.802164Z",
     "shell.execute_reply": "2025-05-18T11:40:52.797964Z",
     "shell.execute_reply.started": "2025-05-18T11:40:48.508264Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "eds_output_path = \"/home/msds2025/jvalera/bdcc2025/bdcc-lab-openlib/partitioned_data/df_eds_partitioned\"\n",
    "exp_output_path = \"/home/msds2025/jvalera/bdcc2025/bdcc-lab-openlib/partitioned_data/df_exp_partitioned\"\n",
    "\n",
    "df_eds = spark.read.parquet(eds_output_path)\n",
    "df_exp = spark.read.parquet(exp_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2024178-2e82-4490-948b-c85dbd45bcce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:40:52.814360Z",
     "iopub.status.busy": "2025-05-18T11:40:52.813781Z",
     "iopub.status.idle": "2025-05-18T11:40:52.855030Z",
     "shell.execute_reply": "2025-05-18T11:40:52.853352Z",
     "shell.execute_reply.started": "2025-05-18T11:40:52.814286Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- decade: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- subject_clean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- decade: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_eds.printSchema()\n",
    "df_exp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53a545-72fb-4982-a14f-105292c4d43e",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4debdd7-b040-457e-99d8-2853e8fad56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:46:31.175458Z",
     "iopub.status.busy": "2025-05-18T12:46:31.174740Z",
     "iopub.status.idle": "2025-05-18T12:46:31.460452Z",
     "shell.execute_reply": "2025-05-18T12:46:31.459862Z",
     "shell.execute_reply.started": "2025-05-18T12:46:31.175398Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Publications All Time: 89,677,650\n"
     ]
    }
   ],
   "source": [
    "# 1. Total Publications All Time\n",
    "total_eds_rows = df_eds.count()\n",
    "print(f\"Total Publications All Time: {total_eds_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d20380d-b442-409a-ac1a-057c67f91339",
   "metadata": {},
   "source": [
    "The 89,677,650 editions reflect a the vast record of human knowledge that captures centuries of intellectual output. This scale underscores publishing’s role as a cultural mirror, given the dataset’s breadth allowing for insights into global trends. However, demands robust computational tools to unlock its potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de5d99d-1413-427e-b7e6-133e807fe642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:40:55.052688Z",
     "iopub.status.busy": "2025-05-18T11:40:55.051978Z",
     "iopub.status.idle": "2025-05-18T11:40:57.906238Z",
     "shell.execute_reply": "2025-05-18T11:40:57.904535Z",
     "shell.execute_reply.started": "2025-05-18T11:40:55.052600Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publications per Decade:\n",
      "+------+--------+\n",
      "|decade|   count|\n",
      "+------+--------+\n",
      "| 1900s| 1211426|\n",
      "| 1910s| 1163794|\n",
      "| 1920s| 1102756|\n",
      "| 1930s| 1202540|\n",
      "| 1940s| 1235263|\n",
      "| 1950s| 1857484|\n",
      "| 1960s| 4471211|\n",
      "| 1970s| 9255754|\n",
      "| 1980s|13551972|\n",
      "| 1990s|19829337|\n",
      "| 2000s|26830021|\n",
      "| 2010s| 7703341|\n",
      "| 2020s|  262751|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Publications per Decade\n",
    "decade_dist = (df_eds\n",
    "               .groupBy(\"decade\")\n",
    "               .count()\n",
    "               .orderBy(\"decade\")\n",
    "              )\n",
    "\n",
    "print(\"Publications per Decade:\")\n",
    "decade_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67906ebe-1e46-48bb-af0a-477e2c925422",
   "metadata": {},
   "source": [
    "The 2000s peak of 26,830,021 editions, compared to 1,211,426 in the 1900s and 262,751 in the 2020s, suggests a digital-era publishing boom. This was likely driven by globalization and technology. The 2020s’ drop hints at data incompleteness since we are still in 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bd7c7b9-c257-400e-a90b-c8998d995cd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:40:57.908643Z",
     "iopub.status.busy": "2025-05-18T11:40:57.908001Z",
     "iopub.status.idle": "2025-05-18T11:41:00.349858Z",
     "shell.execute_reply": "2025-05-18T11:41:00.349008Z",
     "shell.execute_reply.started": "2025-05-18T11:40:57.908586Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Contributing Countries:\n",
      "+-------------+--------+\n",
      "|      country|   count|\n",
      "+-------------+--------+\n",
      "|          USA|27091516|\n",
      "|           UK| 6978204|\n",
      "|       Canada| 1664700|\n",
      "|       Russia|  745596|\n",
      "|        China|  715275|\n",
      "|      Germany|  451786|\n",
      "|        Japan|  330744|\n",
      "|       France|  294949|\n",
      "|        Italy|  264675|\n",
      "|Korea (South)|  180069|\n",
      "|       Israel|  148315|\n",
      "|      Ukraine|  121645|\n",
      "|        Spain|  103632|\n",
      "|        India|  100665|\n",
      "|  Netherlands|   91699|\n",
      "|       Poland|   79553|\n",
      "|  Switzerland|   61071|\n",
      "|        Egypt|   59893|\n",
      "|       Turkey|   58519|\n",
      "|       Mexico|   56401|\n",
      "+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Top 20 Contributing Countries\n",
    "country_dist = (df_eds\n",
    "                .filter(col(\"country\") != \"Unknown\")\n",
    "                .groupBy(\"country\")\n",
    "                .count()\n",
    "                .orderBy(\"count\", ascending=False)\n",
    "                .limit(20)\n",
    "               )\n",
    "\n",
    "print(\"Top 20 Contributing Countries:\")\n",
    "country_dist.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901f6d35-654d-4a9d-9518-63226286a529",
   "metadata": {},
   "source": [
    "The USA and UK’s lead, followed by Canada, Russia, and China, reflects historical and economic advantages in publishing infrastructure. The top 20 contributors also reflect a good mix of countries from sifferent continents of the world. Emerging contributors like Mexico, Turkey, and Egypt signal shifting global dynamics and potential rise in non-Western publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0044e141-65e2-46ef-9891-faea1cfa75a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:41:00.352347Z",
     "iopub.status.busy": "2025-05-18T11:41:00.352010Z",
     "iopub.status.idle": "2025-05-18T11:41:05.916396Z",
     "shell.execute_reply": "2025-05-18T11:41:05.915421Z",
     "shell.execute_reply.started": "2025-05-18T11:41:00.352316Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Velocity for Top 20 Countries (Cumulative Sum):\n",
      "+------+-------------+------+----------------+\n",
      "|decade|      country| count|cumulative_count|\n",
      "+------+-------------+------+----------------+\n",
      "| 1900s|       Canada| 22818|           22818|\n",
      "| 1900s|        China|  1774|            1774|\n",
      "| 1900s|        Egypt|   498|             498|\n",
      "| 1900s|       France| 11359|           11359|\n",
      "| 1900s|      Germany| 19325|           19325|\n",
      "| 1900s|        India|   979|             979|\n",
      "| 1900s|       Israel|   311|             311|\n",
      "| 1900s|        Italy|  2523|            2523|\n",
      "| 1900s|        Japan|  3342|            3342|\n",
      "| 1900s|Korea (South)|   453|             453|\n",
      "| 1900s|       Mexico|   222|             222|\n",
      "| 1900s|  Netherlands|  1172|            1172|\n",
      "| 1900s|       Poland|  1540|            1540|\n",
      "| 1900s|       Russia|  8903|            8903|\n",
      "| 1900s|        Spain|   716|             716|\n",
      "| 1900s|  Switzerland|  1601|            1601|\n",
      "| 1900s|       Turkey|   343|             343|\n",
      "| 1900s|           UK|188973|          188973|\n",
      "| 1900s|          USA|552884|          552884|\n",
      "| 1900s|      Ukraine|  1203|            1203|\n",
      "+------+-------------+------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Publication Velocity for Top 20 Countries (Cumulative Sum)\n",
    "top_20_countries = (country_dist\n",
    "                    .select(\"country\")\n",
    "                    .rdd.flatMap(lambda x: x).collect()\n",
    "                   )\n",
    "\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(\"decade\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "top_20_velocity = (df_eds\n",
    "                   .filter(col(\"country\").isin(top_20_countries))\n",
    "                   .groupBy(\"decade\", \"country\")\n",
    "                   .count()\n",
    "                   .withColumn(\"cumulative_count\", sum_(\"count\").over(window_spec))\n",
    "                   .select(\"decade\", \"country\", \"count\", \"cumulative_count\")\n",
    "                   .orderBy(\"decade\", \"country\")\n",
    "                  )\n",
    "\n",
    "print(\"Publication Velocity for Top 20 Countries (Cumulative Sum):\")\n",
    "top_20_velocity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b63977-751f-4bad-9894-c9433d75c09d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a9e2df-4056-4a56-8ba3-b67ee2d93874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:41:05.919099Z",
     "iopub.status.busy": "2025-05-18T11:41:05.917904Z",
     "iopub.status.idle": "2025-05-18T11:41:08.998025Z",
     "shell.execute_reply": "2025-05-18T11:41:08.996255Z",
     "shell.execute_reply.started": "2025-05-18T11:41:05.919032Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Trends for Top 5 Countries:\n",
      "+------+-------+------+\n",
      "|decade|country| count|\n",
      "+------+-------+------+\n",
      "| 1900s| Canada| 22818|\n",
      "| 1900s|  China|  1774|\n",
      "| 1900s| Russia|  8903|\n",
      "| 1900s|     UK|188973|\n",
      "| 1900s|    USA|552884|\n",
      "| 1910s| Canada| 25801|\n",
      "| 1910s|  China|  2149|\n",
      "| 1910s| Russia|  7299|\n",
      "| 1910s|     UK|175936|\n",
      "| 1910s|    USA|548563|\n",
      "| 1920s| Canada| 16346|\n",
      "| 1920s|  China|  4358|\n",
      "| 1920s| Russia|  6425|\n",
      "| 1920s|     UK|160037|\n",
      "| 1920s|    USA|451649|\n",
      "| 1930s| Canada| 16095|\n",
      "| 1930s|  China| 12722|\n",
      "| 1930s| Russia|  7111|\n",
      "| 1930s|     UK|155653|\n",
      "| 1930s|    USA|470516|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Top 5 Countries' Publication Trends\n",
    "top_5_countries = country_dist.select(\"country\").limit(5).rdd.flatMap(lambda x: x).collect()\n",
    "top_5_trends = (df_eds\n",
    "                .filter(col(\"country\").isin(top_5_countries))\n",
    "                .groupBy(\"decade\", \"country\")\n",
    "                .count()\n",
    "                .orderBy(\"decade\", \"country\"))\n",
    "print(\"Publication Trends for Top 5 Countries:\")\n",
    "top_5_trends.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226f357-0d05-4f79-ac3f-da3613ee8263",
   "metadata": {},
   "source": [
    "The USA (552,884 editions in 1900s) and UK (188,973) dominated early, while China’s rise by the 1930s (12,722) reflects its post-reform publishing surge. These trends mirror geopolitical shifts, with cultural policies and education investments driving output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b300be-9ea3-4a6c-8d34-9e208eba30a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:41:09.000965Z",
     "iopub.status.busy": "2025-05-18T11:41:08.999816Z",
     "iopub.status.idle": "2025-05-18T11:41:20.953963Z",
     "shell.execute_reply": "2025-05-18T11:41:20.952250Z",
     "shell.execute_reply.started": "2025-05-18T11:41:09.000903Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Subjects per Publication: 5.1\n",
      "Distribution of Subjects per Publication:\n",
      "+-------------+--------+----------+\n",
      "|subject_count|   count|proportion|\n",
      "+-------------+--------+----------+\n",
      "|            1| 8820778|      9.84|\n",
      "|            2|13979479|     15.59|\n",
      "|            3|14241023|     15.88|\n",
      "|            4|12875151|     14.36|\n",
      "|            5| 9820838|     10.95|\n",
      "|            6| 7327895|      8.17|\n",
      "|            7| 5369948|      5.99|\n",
      "|            8| 3978704|      4.44|\n",
      "|            9| 2901126|      3.24|\n",
      "|           10| 2280601|      2.54|\n",
      "|           11| 1749826|      1.95|\n",
      "|           12| 1402394|      1.56|\n",
      "|           13| 1123415|      1.25|\n",
      "|           14|  893185|       1.0|\n",
      "|           15|  709268|      0.79|\n",
      "|           16|  554636|      0.62|\n",
      "|           17|  426640|      0.48|\n",
      "|           18|  326683|      0.36|\n",
      "|           19|  241264|      0.27|\n",
      "|           20|  184236|      0.21|\n",
      "+-------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Subjects per Publication\n",
    "df_eds = df_eds.withColumn(\"subject_count\", size(col(\"subject_clean\")))\n",
    "\n",
    "avg_subjects = (df_eds\n",
    "                .withColumn(\"subject_count\", size(col(\"subject_clean\")))\n",
    "                .agg(round(sum_(\"subject_count\") / count(\"*\"), 2).alias(\"avg_subjects_per_work\"))\n",
    "                .first()[\"avg_subjects_per_work\"]\n",
    "               )\n",
    "\n",
    "print(f\"Average Subjects per Publication: {avg_subjects}\")\n",
    "\n",
    "subject_count_dist = (df_eds\n",
    "                      .groupBy(\"subject_count\")\n",
    "                      .count()\n",
    "                      .withColumn(\"proportion\", round((col(\"count\") / lit(total_eds_rows))*100,2))\n",
    "                      .orderBy(\"subject_count\")\n",
    "                     )\n",
    "\n",
    "print(\"Distribution of Subjects per Publication:\")\n",
    "subject_count_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194d72d-412f-4fa1-af70-f75f5f5b66ae",
   "metadata": {},
   "source": [
    "An average of 5.1 subjects could indicate rich thematic complexity and potential interdisciplinarity. This suggests publishers aim to capture diverse audiences as reflected by readers’ different interests. This could also be a signal of the blending of genres over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55185026-f1a6-4e1a-ae08-2c30368f863e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:41:20.956473Z",
     "iopub.status.busy": "2025-05-18T11:41:20.955868Z",
     "iopub.status.idle": "2025-05-18T11:41:20.966980Z",
     "shell.execute_reply": "2025-05-18T11:41:20.965094Z",
     "shell.execute_reply.started": "2025-05-18T11:41:20.956417Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# UDF to generate sorted subject pairs\n",
    "def generate_pairs(subjects):\n",
    "    # Generate all unique pairs and sort each pair\n",
    "    pairs = [sorted(pair) for pair in combinations(subjects, 2)]\n",
    "    # Convert pairs to strings for grouping\n",
    "    return [f\"{pair[0]}|{pair[1]}\" for pair in pairs]\n",
    "\n",
    "generate_pairs_udf = udf(generate_pairs, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8754f9a-f123-4af3-bb64-459ac90f6bbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:12:39.986749Z",
     "iopub.status.busy": "2025-05-18T12:12:39.986032Z",
     "iopub.status.idle": "2025-05-18T12:12:55.246895Z",
     "shell.execute_reply": "2025-05-18T12:12:55.245533Z",
     "shell.execute_reply.started": "2025-05-18T12:12:39.986687Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Subjects Overall:\n",
      "+--------------------+--------+\n",
      "|             subject|   count|\n",
      "+--------------------+--------+\n",
      "|             general|12111120|\n",
      "|             history|10275438|\n",
      "|             fiction| 8184832|\n",
      "|       united states| 6741393|\n",
      "|           biography| 4224031|\n",
      "|        20th century| 2525200|\n",
      "|          congresses| 2232355|\n",
      "| juvenile literature| 2081694|\n",
      "|history and criti...| 2076317|\n",
      "|politics and gove...| 2075446|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Top Subjects Overall\n",
    "top_subjects = df_exp.groupBy(\"subject\").count().orderBy(\"count\", ascending=False)\n",
    "print(\"Top 10 Subjects Overall:\")\n",
    "top_subjects.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a878684-70bf-4c58-a1f4-f0d77bfc8071",
   "metadata": {},
   "source": [
    "History, politics, and economic conditions are at the top 10 subjects which signals humanity’s preoccupation with understanding its past and governance. The prominence of biographies reveals a parallel impulse: the desire to preserve individual legacies and make sense of the human experience through personal narratives. Juvenile literature also makes the list which hints at a collective investment in shaping the minds of the next generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a3d73b9-68b0-4ac1-a5e0-7996f26f15dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:41:20.969632Z",
     "iopub.status.busy": "2025-05-18T11:41:20.968683Z",
     "iopub.status.idle": "2025-05-18T11:50:04.675385Z",
     "shell.execute_reply": "2025-05-18T11:50:04.673480Z",
     "shell.execute_reply.started": "2025-05-18T11:41:20.969575Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Subject Pairings:\n",
      "+------------------------------+-------+\n",
      "|pair                          |count  |\n",
      "+------------------------------+-------+\n",
      "|fiction|general               |3287640|\n",
      "|general|history               |1380267|\n",
      "|history|united states         |1354530|\n",
      "|20th century|history          |1070897|\n",
      "|business|economics            |1049004|\n",
      "|mathematics|science           |931646 |\n",
      "|general|united states         |911516 |\n",
      "|biography|history             |908332 |\n",
      "|general|religion              |863329 |\n",
      "|children's books|general      |827908 |\n",
      "|economics|finance             |811696 |\n",
      "|business|finance              |805341 |\n",
      "|biography|united states       |780971 |\n",
      "|economics|general             |748629 |\n",
      "|business|general              |746244 |\n",
      "|classifiable|non              |740690 |\n",
      "|autobiography|biography       |718474 |\n",
      "|fiction|romance               |700667 |\n",
      "|fiction|juvenile fiction      |696341 |\n",
      "|1939|1945                     |689329 |\n",
      "|1945|world war                |641893 |\n",
      "|1939|world war                |639400 |\n",
      "|business & economics|economics|622453 |\n",
      "|handbooks|manuals             |620459 |\n",
      "|general|juvenile fiction      |616770 |\n",
      "|business|business & economics |616420 |\n",
      "|finance|general               |608234 |\n",
      "|general|reference             |568171 |\n",
      "|etc|handbooks                 |566518 |\n",
      "|biography|general             |564294 |\n",
      "+------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Top Subject Pairings\n",
    "# Generate all unique subject pairs within each row\n",
    "df_pairs = (df_eds\n",
    "            .filter(size(col(\"subject_clean\")) >= 2)  # Ensure at least 2 subjects\n",
    "            .withColumn(\"pair\", explode(generate_pairs_udf(col(\"subject_clean\"))))\n",
    "            .select(\"pair\")\n",
    "           )\n",
    "# Count pair frequencies and get top 10\n",
    "top_pairs = (df_pairs\n",
    "             .groupBy(\"pair\")\n",
    "             .count()\n",
    "             .orderBy(\"count\", ascending=False)\n",
    "             .limit(20)\n",
    "            )\n",
    "\n",
    "print(\"Top 10 Subject Pairings:\")\n",
    "top_pairs.select(\"pair\", \"count\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39d11a-3748-45e8-8dc7-e1c4700c085a",
   "metadata": {},
   "source": [
    "The top subject pairings reveal our core intellectual fixations: a love for storytelling (`fiction|general`), a deep curiosity about the past and power (`history|United States`, `20th century|history`), and a practical obsession with how the world works (`business|economics`, `economics|finance`). Biographies and juvenile fiction signal a human need to preserve personal narratives and shape young minds. Across it all, recurring themes of governance, identity, and knowledge cut through, painting a picture of a world eager to understand the past, present, and future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "508db35d-fcb3-44d7-ab3e-96063ee92ae1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:13:20.064389Z",
     "iopub.status.busy": "2025-05-18T12:13:20.063689Z",
     "iopub.status.idle": "2025-05-18T12:13:33.680991Z",
     "shell.execute_reply": "2025-05-18T12:13:33.679452Z",
     "shell.execute_reply.started": "2025-05-18T12:13:20.064331Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Subjects per Country:\n",
      "+-------------+---------------+\n",
      "|      country|unique_subjects|\n",
      "+-------------+---------------+\n",
      "|          USA|         538717|\n",
      "|           UK|         241992|\n",
      "|       Canada|         148952|\n",
      "|      Germany|          90269|\n",
      "|       France|          69661|\n",
      "|       Russia|          66924|\n",
      "|        China|          49722|\n",
      "|        Italy|          48334|\n",
      "|        Japan|          46105|\n",
      "|  Netherlands|          31548|\n",
      "|        Spain|          31102|\n",
      "|  Switzerland|          28116|\n",
      "|      Ukraine|          25959|\n",
      "|       Poland|          25524|\n",
      "|       Israel|          23888|\n",
      "|        India|          21057|\n",
      "|       Sweden|          21012|\n",
      "|Korea (South)|          20112|\n",
      "|       Mexico|          19456|\n",
      "|      Belgium|          18700|\n",
      "+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Unique Subjects per Country\n",
    "unique_subjects_country = (df_exp\n",
    "                           .filter(col(\"country\") != \"Unknown\")\n",
    "                           .groupBy(\"country\")\n",
    "                           .agg(countDistinct(\"subject\").alias(\"unique_subjects\"))\n",
    "                           .orderBy(\"unique_subjects\", ascending=False)\n",
    "                          )\n",
    "\n",
    "print(\"Unique Subjects per Country:\")\n",
    "unique_subjects_country.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a8cf7-a58f-4bb6-8598-cb1b1c6009a2",
   "metadata": {},
   "source": [
    "The U.S. leads by a landslide in subject diversity which reflects its vast publishing industry and cultural exports. The UK and Canada follow which signals strong literary infrastructures and global influence. European countries like Germany, France, and Russia show deep intellectual traditions, while rising entries from China, India, and South Korea point to growing academic and cultural output. Overall, the spread shows that topics many topics can shape not just what’s written, but what’s worth writing about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c561fc0-8abb-4f41-95c5-8ec9036426f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:50:46.734720Z",
     "iopub.status.busy": "2025-05-18T11:50:46.734122Z",
     "iopub.status.idle": "2025-05-18T11:51:10.867463Z",
     "shell.execute_reply": "2025-05-18T11:51:10.865868Z",
     "shell.execute_reply.started": "2025-05-18T11:50:46.734650Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Subjects per Decade:\n",
      "+------+---------------+\n",
      "|decade|unique_subjects|\n",
      "+------+---------------+\n",
      "| 1900s|          99042|\n",
      "| 1910s|          90662|\n",
      "| 1920s|          94612|\n",
      "| 1930s|         106692|\n",
      "| 1940s|         102437|\n",
      "| 1950s|         139706|\n",
      "| 1960s|         237205|\n",
      "| 1970s|         391996|\n",
      "| 1980s|         496153|\n",
      "| 1990s|         603020|\n",
      "| 2000s|         753107|\n",
      "| 2010s|         276002|\n",
      "| 2020s|          30084|\n",
      "+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Unique Subjects per Decade\n",
    "unique_subjects_decade = (df_exp\n",
    "                          .groupBy(\"decade\")\n",
    "                          .agg(countDistinct(\"subject\").alias(\"unique_subjects\"))\n",
    "                          .orderBy(\"decade\")\n",
    "                         )\n",
    "\n",
    "print(\"Unique Subjects per Decade:\")\n",
    "unique_subjects_decade.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00b070-8c82-495e-86d9-9b84d0732804",
   "metadata": {},
   "source": [
    "Unique subjects have exploded since the mid-20th century. The 1950s marked the jumpstart, but growth really took off from the 1960s onward. The 1990s and 2000s hit peak diversity, probably because of technology, globalization, and new fields. The dip in the 2010s and 2020s likely shows delayed cataloging or shifting focus, but the overall trend is giving expansion and evolution in knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eeb1fa6-180d-4de0-86f3-7cb1ed976b70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T11:51:10.869902Z",
     "iopub.status.busy": "2025-05-18T11:51:10.869282Z",
     "iopub.status.idle": "2025-05-18T12:07:07.505103Z",
     "shell.execute_reply": "2025-05-18T12:07:07.503624Z",
     "shell.execute_reply.started": "2025-05-18T11:51:10.869846Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects First Emerging per Decade:\n",
      "1920s: [\"'s lands plantentuin\", '(1928)', '(cornwall', '+kwangtung', '102nd co a', '1064', '1090', '1367?', '1492?', '1527or 8']\n",
      "1930s: ['\" 1824', '\"hét\"', '\"reprinted from the journal of the american chemical society', '(1638)', '(bulgaria)', '+anonyms and pseudonyms', '1000(ca)', '120', '1328 (capétiens)', '1497 or 8']\n",
      "1940s: ['(1909', '(1941)', ') class of 1938', '1015 or 16', '1060?', '129th infantry', '1345?', '1539 (ca)', '1575 history of asia', '1579)']\n",
      "1950s: ['\"beat\" culture', '(51)', '(dalmatia)', ') bibliothèque', ') torgovoe predstavitelśtvo v irane', '0822 medieval philosophy', '1029', '1170?', '1423?', '15th battalion']\n",
      "1960s: ['\"clarté\" (ligue de solidarité intellectuelle pour le triomphe de la cause internationale)', '\"nibelungenlied\"', \"'atāhiyah\", '(1643)', '(1678', '(george catlett)', '(nikolai)', '0 gesamtdarstellungen', '101 ou 100', '105 b c']\n",
      "1970s: ['!xõ language', '\"želsvelės\" kolūkis (lithuania)', \"'arabiyah\", \"'od\", \"'ubaid\", '(1842', '(1915 :', '(1975)', '(burlington', '(charles haddon)']\n",
      "1980s: ['\" 7', '\"68\"', '\"max frisch', '\"proletarskoe iskusstvo\" (factory)', '\"viti︠a︡zʹ\" (ship)', '$x', \"'broṅ pa cho dgu\", '(10th :', '(27th :', '(andrew barton)']\n",
      "1990s: ['\\x1bsara (buddhism)', '\"ciel mon mardi!\" (émission télévisée)', '\"didache \"', '\"plast\" orhanizat͡s︡ii͡a︡ ukraïnsʹkoï molodi plastova stanyt͡s︡i͡a︡ ditroĭt', '\"thalassa\" (émission télévisée)', '\"tractatus de primo principio\"', '& dunloy credit union', '& phoenix railway', \"'cd\", \"'etudes de\"]\n",
      "2000s: ['\"farce de maître pierre pathelin\"', '\"i͡uridicheskai͡a literatura\" (firm)', '\"prosvita\" (krolevet︠s︡ʹ', '\"rulʹ\" (berlin', '\"the forks of the delaware\"', \"'mental control': how you can master your mind and body\", '( g )', '(1835', '(1999 :', '(2000)']\n",
      "2010s: ['\"gemeindeschlüssel de05170028\"', '\"jeu d\\'adam\"', '\"zhong yong \"', '\"слово о полку игореве\"', '& word lists', '(electronic resource)', ') omskoe regionalʹnoe otdelenie', ') turkey)', \"03 hŭimang sŏul chŏngch'aek pangnamhoe (2013 : seoul\", '04 world war (1939']\n",
      "2020s: ['3751', '8921 fascism', '> total quality management', 'achille cutrera', 'agaves in literature', 'agents de brevets', 'anarchisme dans la littérature', 'and sanctions movement', 'approximately 1548', 'associazione contemporary locus']\n"
     ]
    }
   ],
   "source": [
    "# 11. Subjects First Emerging per Decade\n",
    "subject_first_decade = (df_exp\n",
    "                        .groupBy(\"subject\")\n",
    "                        .agg(min_(\"decade\").alias(\"first_decade\"))\n",
    "                        .orderBy(\"first_decade\")\n",
    "                       )\n",
    "\n",
    "emerging_subjects = {}\n",
    "for decade in [\"1920s\", \"1930s\", \"1940s\", \"1950s\", \"1960s\", \"1970s\", \"1980s\", \"1990s\", \"2000s\", \"2010s\", \"2020s\"]:\n",
    "    subjects = (subject_first_decade\n",
    "                .filter(col(\"first_decade\") == decade)\n",
    "                .select(\"subject\")\n",
    "                .limit(10)\n",
    "                .rdd.flatMap(lambda x: x)\n",
    "                .collect()\n",
    "               )\n",
    "    emerging_subjects[decade] = subjects\n",
    "    \n",
    "print(\"Subjects First Emerging per Decade:\")\n",
    "for decade, subjects in emerging_subjects.items():\n",
    "    print(f\"{decade}: {subjects}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154865d-b78d-4b1c-8b3b-e28455616cf4",
   "metadata": {},
   "source": [
    "Each decade drops its own unique flavor of new subjects which is a sign of shifting interests and cultural shifts. The 1920s and ’30s lean heavily into history, geography, and names. The post-war decades, especially the ’50s and ’60s, bring culture, language, and political movements front and center. From the ’70s onward, diversity explodes, with languages, TV shows, political groups, tech, and even music sneaking in. The 2000s and 2010s reflect digitization and globalization, while the 2020s are just starting to show fresh themes, including quality management and social movements. It’s a timeline of evolving human curiosity, one subject at a time.\n",
    "\n",
    "*Note: ChatGPT was asked to translate most of the output above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a2ebdaf-03b3-41e9-bad1-767d997f9bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T12:07:07.507405Z",
     "iopub.status.busy": "2025-05-18T12:07:07.506800Z",
     "iopub.status.idle": "2025-05-18T12:07:31.356080Z",
     "shell.execute_reply": "2025-05-18T12:07:31.354390Z",
     "shell.execute_reply.started": "2025-05-18T12:07:07.507350Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Subjects by Number of Countries:\n",
      "+--------------------+-------------+\n",
      "|             subject|num_countries|\n",
      "+--------------------+-------------+\n",
      "|             history|          249|\n",
      "|politics and gove...|          226|\n",
      "| economic conditions|          225|\n",
      "|          statistics|          222|\n",
      "|          congresses|          221|\n",
      "|social life and c...|          220|\n",
      "|           biography|          217|\n",
      "|   social conditions|          216|\n",
      "|          population|          212|\n",
      "|               women|          210|\n",
      "+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 12. Subject Overlap Across Countries\n",
    "country_subject_counts = (df_exp\n",
    "                          .groupBy(\"country\", \"subject\")\n",
    "                          .count()\n",
    "                         )\n",
    "\n",
    "subject_countries = (country_subject_counts\n",
    "                     .groupBy(\"subject\")\n",
    "                     .agg(countDistinct(\"country\").alias(\"num_countries\"))\n",
    "                     .orderBy(\"num_countries\", ascending=False)\n",
    "                    )\n",
    "\n",
    "print(\"Top 10 Subjects by Number of Countries:\")\n",
    "subject_countries.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65141090-8de8-40f3-8086-6ec0e08669df",
   "metadata": {},
   "source": [
    "Top subjects like history, politics, and economic conditions show up everywhere. Almost every country is obsessed with these. Statistics, social life, and biographies follow close behind which proves that humans everywhere crave context and stories about people. These show that globally, publications tend to focus on power, society, and identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaaad97-0aef-4513-b1d1-511a7a0412c0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-18T07:28:39.822829Z",
     "iopub.status.idle": "2025-05-18T07:28:39.823062Z",
     "shell.execute_reply": "2025-05-18T07:28:39.822957Z",
     "shell.execute_reply.started": "2025-05-18T07:28:39.822947Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Convert to Pandas for Visualizations\n",
    "# decade_dist_pd = decade_dist.toPandas()\n",
    "# country_dist_pd = country_dist.toPandas()\n",
    "# top_country_velocity_pd = top_country_velocity.toPandas()\n",
    "# top_5_trends_pd = top_5_trends.toPandas()\n",
    "# subject_count_dist_pd = subject_count_dist.toPandas()\n",
    "# frequent_itemsets_pd = frequent_itemsets.toPandas()\n",
    "# top_subjects_pd = top_subjects.toPandas()\n",
    "# unique_subjects_country_pd = unique_subjects_country.toPandas()\n",
    "# unique_subjects_decade_pd = unique_subjects_decade.toPandas()\n",
    "# subject_countries_pd = subject_countries.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc1d13b-68fd-4d9a-95c1-3d0a08e3c470",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This study presents a comprehensive, data-driven exploration of global publishing trends from the 1900s to the 2020s using the Open Library dataset and Apache Spark for large-scale data processing. The findings illuminate the thematic and geographic dimensions of global literary production that revealed key patterns such as:\n",
    "\n",
    "- A surge in publishing volume in the 2000s, followed by a slight decline.\n",
    "\n",
    "- Dominance of English-speaking countries, particularly the United States and the United Kingdom.\n",
    "\n",
    "- An average of 5.1 subjects per publication, with \"History\" and \"Biography\" emerging as near-universal themes across regions.\n",
    "\n",
    "- The increasing thematic complexity of books over time, evidenced by the rise in subject co-occurrences.\n",
    "\n",
    "These results underscore the value of bibliographic big data in tracing the evolution of global knowledge and intellectual discourse.\n",
    "\n",
    "## Recommendations\n",
    "1. Future studies should explore publication trends at a finer temporal resolution (e.g., by decade or 5-year intervals) to identify cultural shifts linked to historical events (e.g., wars, economic booms, or pandemics).\n",
    "\n",
    "2. Augment the analysis by linking author metadata (e.g., nationality, gender, and birth/death years) to better understand representation and voice in global literature.\n",
    "\n",
    "3. Consider filtering or separately analyzing non-English publications to reveal insights masked by English-language dominance in the dataset.\n",
    "\n",
    "4. Apply NLP techniques like LDA or BERTopic to surface latent themes and their evolution, beyond the fixed subject tags.\n",
    "\n",
    "5. Map publishing patterns against global socio-political or economic timelines to contextualize spikes or drops in specific subjects or geographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d9e9e0-e7c0-4b87-9a0b-6bf03b99df9f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
